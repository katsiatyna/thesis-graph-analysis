{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/public/spark-0.9.1/bin/pyspark\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from pynauty import *\n",
    "import re\n",
    "import networkx as nx\n",
    "from hdfs import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subgraph_hash(edges):\n",
    "    # ORDER INSIDE EDGES IS ALREADY DONE\n",
    "    # NOW ONLY NEED TO ORDER EDGES BETWEEN THEMSELVES\n",
    "    hash_str = ''\n",
    "    # rearrange the edges\n",
    "    edges = sorted(edges)  # sorts on the first elements first and then on second\n",
    "    for edge in edges:\n",
    "        hash_str += str(edge[0]) + '.' + str(edge[1]) + ';'\n",
    "    return hash_str\n",
    "\n",
    "def map_to_graph(combination, edges):\n",
    "    # edges are list of tuples\n",
    "\n",
    "    # this is going to be the key\n",
    "    original_edges = list()\n",
    "    if type(combination) is int:\n",
    "        combination = [combination]\n",
    "    for ind in combination:\n",
    "        original_edges.append(edges[ind])\n",
    "    original_vertices = set()\n",
    "    for edge in original_edges:\n",
    "        original_vertices.add(edge[0])\n",
    "        original_vertices.add(edge[1])\n",
    "    original_vertices = list(original_vertices)\n",
    "    g = Graph(len(original_vertices))\n",
    "    new_edges = list()\n",
    "    for edge in original_edges:\n",
    "        g.connect_vertex(original_vertices.index(edge[0]), original_vertices.index(edge[1]))\n",
    "        new_edges.append((original_vertices.index(edge[0]), original_vertices.index(edge[1])))\n",
    "    # v_g = VsigramGraph(g, None, label_arr=canon_label(g), orig_edges=original_edges)\n",
    "    # subgraph_collection = SubgraphCollection(v_g.label_arr, subgraphs=[v_g], freq=1)\n",
    "    return (canon_label(g), (1, [original_edges]))\n",
    "\n",
    "\n",
    "def update_subgraph_freq(a, b):\n",
    "    # freq_object = SubgraphCollection(label=a.label)\n",
    "    # freq_object.subgraphs = a.subgraphs + b.subgraphs\n",
    "    # freq_object.freq = a.freq + b.freq\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "\n",
    "def filter_by_connected(edges_indexes, orig_edges):\n",
    "    # create networkx graph\n",
    "    first_element = True\n",
    "    edge_new = edges_indexes[len(edges_indexes) - 1]\n",
    "    edges_old = list(edges_indexes[0:len(edges_indexes) - 1]) if type(edges_indexes[0:len(edges_indexes) - 1]) is list \\\n",
    "        else [edges_indexes[0:len(edges_indexes) - 2]]\n",
    "    # print str(edges_old) + str(type(edges_old))\n",
    "    # print str(edge_new) + str(type(edge_new))\n",
    "    if first_element:\n",
    "        # print 'INDEXES: ' + str(edges_indexes) + str(type(edges_indexes))\n",
    "        # print 'OLD: ' + str(edges_old) + str(type(edges_old))\n",
    "        # print 'NEW: ' + str(edge_new) + str(type(edge_new))\n",
    "        # print str(edge_new in edges_old)\n",
    "        first_element = False\n",
    "    if edge_new in edges_old:\n",
    "        # print 'REPEATING EDGE'\n",
    "        return False\n",
    "    edges_list = list()\n",
    "    for ind in list(edges_indexes):\n",
    "        edges_list.append(orig_edges[ind])\n",
    "    # print edges_list\n",
    "    nx_g = nx.Graph()\n",
    "    nx_g.add_edges_from(edges_list)\n",
    "    return nx.is_connected(nx_g)\n",
    "\n",
    "\n",
    "def mapToList(edges_indexes):\n",
    "    tupl_to_list = list(edges_indexes[0]) if type(edges_indexes[0]) is list else [edges_indexes[0]]\n",
    "    tupl_to_list.append(edges_indexes[1])\n",
    "    # print 'LIST: ' + str(tupl_to_list)\n",
    "    return tupl_to_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting HDFS directory...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=SubgraphMining, master=yarn) created by __init__ at <ipython-input-5-2f179219cc00>:8 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f179219cc00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# conf = SparkConf().setAppName('SubgraphMining').setMaster('spark://bscdc01.dc.bsc:7077')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# sc = SparkContext(conf=conf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yarn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SubgraphMining\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=SubgraphMining, master=yarn) created by __init__ at <ipython-input-5-2f179219cc00>:8 "
     ]
    }
   ],
   "source": [
    "# conf = SparkConf().setAppName('SubgraphMining').setMaster('spark://bscdc01.dc.bsc:7077')\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc = SparkContext(\"yarn\", \"SubgraphMining\")\n",
    "print sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting HDFS directory...\n",
      "578\n",
      "578\n",
      "SIZE 2\n",
      "(0, 0)\n",
      "[0, 18]\n",
      "SIZE 3\n",
      "([0, 18], 0)\n",
      "[1, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "#dfs_root = 'hdfs://bscdc01:54310/'\n",
    "client = Config().get_client('dev')\n",
    "print 'Deleting HDFS directory...'\n",
    "client.delete('subgraphs', recursive=True)\n",
    "sample = '7d734d06-f2b1-4924-a201-620ac8084c49'\n",
    "\n",
    "# try to read from hdfs\n",
    "lines = sc.textFile('samples/new_assignment_separate.csv')\n",
    "header = lines.first()  # extract header\n",
    "lines = lines.filter(lambda row: row != header)   # filter out header\n",
    "positions_rdd = lines.map(lambda line: line.split(','))\n",
    "# print positions_rdd.collect()\n",
    "positions_combined = lines.flatMap(lambda line: line.split(','))\n",
    "positions_distinct = positions_combined.distinct()\n",
    "positions_distinct_list = positions_distinct.collect()\n",
    "edges_rdd = positions_rdd.map(lambda positions: [positions_distinct_list.index(positions[0])\n",
    "                                                 if positions_distinct_list.index(positions[0]) < positions_distinct_list.index(positions[1])\n",
    "                                                 else positions_distinct_list.index(positions[1]),\n",
    "                                                 positions_distinct_list.index(positions[1])\n",
    "                                                 if positions_distinct_list.index(positions[0]) < positions_distinct_list.index(positions[1])\n",
    "                                                 else positions_distinct_list.index(positions[0])])\n",
    "edges_rdd = edges_rdd.map(lambda edge: (str(edge[0]) + ':' + str(edge[1]), (edge, 1)))\n",
    "# print edges_rdd.collect()\n",
    "edges_rdd = edges_rdd.reduceByKey(lambda edge_kv1, edge_kv2: (edge_kv1[0], edge_kv1[1] + edge_kv2[1]) )\n",
    "edges_rdd_list = edges_rdd.collect()\n",
    "edges = [tuple(item[1][0]) for item in edges_rdd_list]\n",
    "print len(edges)\n",
    "\n",
    "rdd_1 = sc.parallelize(range(len(edges)))\n",
    "rdd_of_graphs_1 = rdd_1.map(lambda combination: map_to_graph(combination, edges))\n",
    "#     rdd_filtered = rdd_of_graphs.filter(lambda x: x[1] is not None)\n",
    "counts_by_label_1 = rdd_of_graphs_1.reduceByKey(lambda a, b: update_subgraph_freq(a, b))\n",
    "counts_by_label_list_1 = counts_by_label_1.collect()\n",
    "counts_by_label_1.saveAsTextFile('subgraphs/' + sample + '/' + str(1))\n",
    "rdd_last = rdd_1\n",
    "print rdd_1.count()\n",
    "\n",
    "for i in range(2, 4):\n",
    "    print 'SIZE ' + str(i)\n",
    "    # for each element in rdd_1 create a list and add to new rdd\n",
    "    rdd_next = rdd_last.cartesian(rdd_1)\n",
    "    print rdd_next.first()\n",
    "    # rdd_next = rdd_next.flatMap(lambda x: [element for tupl in x for element in tupl])\n",
    "    # filter the connected graphs\n",
    "    rdd_next = rdd_next.map(lambda x: mapToList(x))\n",
    "    rdd_next = rdd_next.filter(lambda comb: filter_by_connected(comb, edges))\n",
    "\n",
    "    print rdd_next.first()\n",
    "    rdd_last = rdd_next\n",
    "    # create a graph from each list\n",
    "    rdd_of_graphs = rdd_next.map(lambda combination: map_to_graph(combination, edges))\n",
    "#     rdd_filtered = rdd_of_graphs.filter(lambda x: x[1] is not None)\n",
    "    counts_by_label = rdd_of_graphs.reduceByKey(lambda a, b: update_subgraph_freq(a, b))\n",
    "    counts_by_label.saveAsTextFile('subgraphs/' + sample + '/' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
