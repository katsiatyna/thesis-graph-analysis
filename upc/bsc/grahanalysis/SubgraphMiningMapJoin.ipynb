{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from pynauty import *\n",
    "import re\n",
    "import networkx as nx\n",
    "from hdfs import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subgraph_hash(edges_indexes):\n",
    "    # ORDER INSIDE EDGES IS ALREADY DONE\n",
    "    # NOW ONLY NEED TO ORDER EDGES BETWEEN THEMSELVES\n",
    "    hash_str = ''\n",
    "    # rearrange the edges\n",
    "    edges_indexes = sorted(edges_indexes)  # sorts on the first elements first and then on second\n",
    "    for edge in edges_indexes:\n",
    "        hash_str += str(edge) + ';'\n",
    "    return hash_str\n",
    "\n",
    "\n",
    "def map_to_tuple_with_hash(combination):\n",
    "    # edges are list of indexes\n",
    "    # this is going to be the key\n",
    "    if type(combination) is int:\n",
    "        combination = [combination]\n",
    "    return (get_subgraph_hash(combination), combination)\n",
    "\n",
    "\n",
    "\n",
    "def map_to_graph(combination, edges):\n",
    "    # edges are list of tuples\n",
    "\n",
    "    # this is going to be the key\n",
    "    original_edges = list()\n",
    "    if type(combination) is int:\n",
    "        combination = [combination]\n",
    "    for ind in combination:\n",
    "        original_edges.append(edges.value[ind])\n",
    "    original_vertices = set()\n",
    "    for edge in original_edges:\n",
    "        original_vertices.add(edge[0])\n",
    "        original_vertices.add(edge[1])\n",
    "    original_vertices = list(original_vertices)\n",
    "    g = Graph(len(original_vertices))\n",
    "    new_edges = list()\n",
    "    for edge in original_edges:\n",
    "        g.connect_vertex(original_vertices.index(edge[0]), original_vertices.index(edge[1]))\n",
    "        new_edges.append((original_vertices.index(edge[0]), original_vertices.index(edge[1])))\n",
    "    return (canon_label(g), (1, [original_edges]))\n",
    "\n",
    "\n",
    "def update_subgraph_freq(a, b):\n",
    "    # freq_object = SubgraphCollection(label=a.label)\n",
    "    # freq_object.subgraphs = a.subgraphs + b.subgraphs\n",
    "    # freq_object.freq = a.freq + b.freq\n",
    "    return (a[0] + b[0], a[1] + b[1])\n",
    "\n",
    "\n",
    "def filter_by_connected(edges_indexes, orig_edges):\n",
    "    # create networkx graph\n",
    "    first_element = True\n",
    "    edge_new = edges_indexes[len(edges_indexes) - 1]\n",
    "    edges_old = list(edges_indexes[0:len(edges_indexes) - 1]) if type(edges_indexes[0:len(edges_indexes) - 1]) is list \\\n",
    "        else [edges_indexes[0:len(edges_indexes) - 2]]\n",
    "    # print str(edges_old) + str(type(edges_old))\n",
    "    # print str(edge_new) + str(type(edge_new))\n",
    "    if first_element:\n",
    "        # print 'INDEXES: ' + str(edges_indexes) + str(type(edges_indexes))\n",
    "        # print 'OLD: ' + str(edges_old) + str(type(edges_old))\n",
    "        # print 'NEW: ' + str(edge_new) + str(type(edge_new))\n",
    "        # print str(edge_new in edges_old)\n",
    "        first_element = False\n",
    "    if edge_new in edges_old:\n",
    "        # print 'REPEATING EDGE'\n",
    "        return False\n",
    "    edges_list = list()\n",
    "    for ind in list(edges_indexes):\n",
    "        edges_list.append(orig_edges[ind])\n",
    "    # print edges_list\n",
    "    nx_g = nx.Graph()\n",
    "    nx_g.add_edges_from(edges_list)\n",
    "    return nx.is_connected(nx_g)\n",
    "\n",
    "\n",
    "def mapToList(edges_indexes):\n",
    "    tupl_to_list = list(edges_indexes[0]) if type(edges_indexes[0]) is list else [edges_indexes[0]]\n",
    "    tupl_to_list.append(edges_indexes[1])\n",
    "    # print 'LIST: ' + str(tupl_to_list)\n",
    "    return tupl_to_list\n",
    "\n",
    "\n",
    "def join_connected_edges(combination, original_edges):\n",
    "    edges_indexes_list = list(combination) if type(combination) is list else [combination]\n",
    "    result_list = []\n",
    "    combination_edges = []\n",
    "    vertices_set = set()\n",
    "    for edge_index in edges_indexes_list:\n",
    "        edge_current = original_edges.value[edge_index]\n",
    "        combination_edges.append(edge_current)\n",
    "        vertices_set.add(edge_current[0])\n",
    "        vertices_set.add(edge_current[1])\n",
    "    for edge in original_edges.value:\n",
    "        if edge not in combination_edges and (edge[0] in vertices_set or edge[1] in vertices_set):\n",
    "            new_list = edges_indexes_list + [original_edges.value.index(edge)]\n",
    "            # print new_list\n",
    "            result_list.append(new_list)\n",
    "    # print 'LISTS: ' + str(result_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f17d71fffd0>\n"
     ]
    }
   ],
   "source": [
    "# sc.stop()\n",
    "sc = SparkContext(\"yarn\", \"SubgraphMiningMapJoin\")\n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting HDFS directory...\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "# try to read from hdfs\n",
    "client = Config().get_client('dev')\n",
    "print 'Deleting HDFS directory...'\n",
    "sample = '7d734d06-f2b1-4924-a201-620ac8084c49'\n",
    "client.delete('subgraphs/' + sample, recursive=True)\n",
    "\n",
    "\n",
    "lines = sc.textFile('samples/new_assignment_separate.csv')\n",
    "header = lines.first()  # extract header\n",
    "lines = lines.filter(lambda row: row != header)   # filter out header\n",
    "positions_rdd = lines.map(lambda line: line.split(','))\n",
    "# print positions_rdd.collect()\n",
    "positions_combined = lines.flatMap(lambda line: line.split(','))\n",
    "positions_distinct = positions_combined.distinct()\n",
    "positions_distinct_list = positions_distinct.collect()\n",
    "edges_rdd = positions_rdd.map(lambda positions: [positions_distinct_list.index(positions[0])\n",
    "                                                 if positions_distinct_list.index(positions[0]) < positions_distinct_list.index(positions[1])\n",
    "                                                 else positions_distinct_list.index(positions[1]),\n",
    "                                                 positions_distinct_list.index(positions[1])\n",
    "                                                 if positions_distinct_list.index(positions[0]) < positions_distinct_list.index(positions[1])\n",
    "                                                 else positions_distinct_list.index(positions[0])])\n",
    "edges_rdd = edges_rdd.map(lambda edge: (str(edge[0]) + ':' + str(edge[1]), (edge, 1)))\n",
    "# print edges_rdd.collect()\n",
    "edges_rdd = edges_rdd.reduceByKey(lambda edge_kv1, edge_kv2: (edge_kv1[0], edge_kv1[1] + edge_kv2[1]) )\n",
    "edges_rdd_list = edges_rdd.collect()\n",
    "edges = [tuple(item[1][0]) for item in edges_rdd_list]\n",
    "print len(edges)\n",
    "edges_list = sc.broadcast(edges)\n",
    "\n",
    "rdd_1 = sc.parallelize(range(len(edges)))\n",
    "# print rdd_1.collect()\n",
    "rdd_of_graphs_1 = rdd_1.map(lambda combination: map_to_graph(combination, edges_list))\n",
    "#     rdd_filtered = rdd_of_graphs.filter(lambda x: x[1] is not None)\n",
    "counts_by_label_1 = rdd_of_graphs_1.reduceByKey(lambda a, b: update_subgraph_freq(a, b))\n",
    "counts_by_label_list_1 = counts_by_label_1.collect()\n",
    "counts_by_label_1.saveAsTextFile('subgraphs/' + sample + '/' + str(1))\n",
    "rdd_last = rdd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE 2\n",
      "[0, 13]\n",
      "11360\n",
      "('0;13;', [0, 13])\n",
      "5680\n",
      "[44, 360]\n",
      "('  0:  2;  1:  2;  2:  0 1;', (1, [[(96, 140), (58, 96)]]))\n",
      "SIZE 3\n",
      "[452, 494, 39]\n",
      "204046\n",
      "('61;191;322;', [191, 322, 61])\n",
      "87446\n",
      "[148, 395, 102]\n",
      "('  0:  3;  1:  3;  2:  3;  3:  0 1 2;', (1, [[(64, 123), (64, 135), (64, 134)]]))\n",
      "SIZE 4\n",
      "[148, 395, 102, 3]\n",
      "4460385\n",
      "('3;102;148;395;', [148, 395, 102, 3])\n",
      "1631059\n",
      "[148, 44, 281, 42]\n",
      "('  0:  4;  1:  4;  2:  3;  3:  2 4;  4:  0 1 3;', (1, [[(123, 148), (64, 123), (47, 64), (64, 182)]]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 5):\n",
    "    print 'SIZE ' + str(i)\n",
    "    # for each element in rdd_1 create a list and add to new rdd\n",
    "    rdd_next = rdd_last.flatMap(lambda combination: join_connected_edges(combination, edges_list))\n",
    "    print rdd_next.first()\n",
    "    rdd_next = rdd_next.map(lambda combination: map_to_tuple_with_hash(combination))\n",
    "    print rdd_next.count()\n",
    "    print rdd_next.first()\n",
    "    rdd_next = rdd_next.reduceByKey(lambda a, b: a)\n",
    "    print rdd_next.count()\n",
    "    rdd_next = rdd_next.map(lambda x: x[1])\n",
    "    print rdd_next.first()\n",
    "    rdd_last = rdd_next\n",
    "    # create a graph from each list\n",
    "    rdd_of_graphs = rdd_next.map(lambda combination: map_to_graph(combination, edges_list))\n",
    "    \n",
    "    print rdd_of_graphs.first()\n",
    "    counts_by_label = rdd_of_graphs.reduceByKey(lambda a, b: update_subgraph_freq(a, b))\n",
    "    #print counts_by_label.first()\n",
    "    counts_by_label.saveAsTextFile('subgraphs/' + sample + '/' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
